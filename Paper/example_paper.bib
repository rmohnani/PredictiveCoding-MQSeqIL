@inproceedings{alonso2024understanding,
  title={Understanding and improving optimization in predictive coding networks},
  author={Alonso, Nicholas and Krichmar, Jeffrey and Neftci, Emre},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  year={2024}
}


@InBook{LeCuBottOrrMull9812,
  author    = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  pages     = {9--48},
  publisher = {Springer Berlin Heidelberg},
  title     = {Efficient BackProp},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-35289-8},
  abstract  = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.},
  booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
  doi       = {10.1007/978-3-642-35289-8_3},
  url       = {https://doi.org/10.1007/978-3-642-35289-8_3},
}

@article{orchard2019making,
  title={Making predictive coding networks generative},
  author={Orchard, Jeff and Sun, Wei},
  journal={arXiv preprint arXiv:1910.12151},
  year={2019}
}

@inproceedings{labelme50k,
author = {Uetz, Rafael and Behnke, Sven},
year = {2009},
month = {12},
pages = {536 - 541},
title = {Large-scale Object Recognition with CUDA-accelerated Hierarchical Neural Networks},
volume = {1},
doi = {10.1109/ICICISYS.2009.5357786}
}

@article{caltech256,
author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
year = {2007},
month = {03},
pages = {},
title = {Caltech-256 Object Category Dataset},
journal = {CalTech Report}
}

@article{cifar100,
title= {CIFAR-100 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).
}}
